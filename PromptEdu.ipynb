{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Setup Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (0.18.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from groq) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from groq) (2.10.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.27.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-stack in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (0.1.5.1)\n",
      "Requirement already satisfied: blobfile in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from llama-stack) (3.0.0)\n",
      "Requirement already satisfied: fire in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from llama-stack) (0.7.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from llama-stack) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from llama-stack) (0.29.1)\n",
      "Requirement already satisfied: jsonschema in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from llama-stack) (4.23.0)\n",
      "Requirement already satisfied: llama-models>=0.1.5rc3 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from llama-stack) (0.1.5)\n",
      "Requirement already satisfied: llama-stack-client>=0.1.5rc3 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from llama-stack) (0.1.5)\n",
      "Requirement already satisfied: prompt-toolkit in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from llama-stack) (3.0.50)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from llama-stack) (1.0.1)\n",
      "Requirement already satisfied: pydantic>=2 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from llama-stack) (2.10.6)\n",
      "Requirement already satisfied: requests in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from llama-stack) (2.32.3)\n",
      "Requirement already satisfied: rich in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from llama-stack) (13.9.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from llama-stack) (65.5.0)\n",
      "Requirement already satisfied: termcolor in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from llama-stack) (2.5.0)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from llama-models>=0.1.5rc3->llama-stack) (6.0.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from llama-models>=0.1.5rc3->llama-stack) (3.1.5)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from llama-models>=0.1.5rc3->llama-stack) (0.9.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from llama-models>=0.1.5rc3->llama-stack) (11.1.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from llama-stack-client>=0.1.5rc3->llama-stack) (4.8.0)\n",
      "Requirement already satisfied: click in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from llama-stack-client>=0.1.5rc3->llama-stack) (8.1.8)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from llama-stack-client>=0.1.5rc3->llama-stack) (1.9.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from llama-stack-client>=0.1.5rc3->llama-stack) (2.2.3)\n",
      "Requirement already satisfied: pyaml in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from llama-stack-client>=0.1.5rc3->llama-stack) (25.1.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from llama-stack-client>=0.1.5rc3->llama-stack) (1.3.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from llama-stack-client>=0.1.5rc3->llama-stack) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from llama-stack-client>=0.1.5rc3->llama-stack) (4.12.2)\n",
      "Requirement already satisfied: certifi in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from httpx->llama-stack) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from httpx->llama-stack) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from httpx->llama-stack) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from httpcore==1.*->httpx->llama-stack) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from pydantic>=2->llama-stack) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from pydantic>=2->llama-stack) (2.27.2)\n",
      "Requirement already satisfied: pycryptodomex>=3.8 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from blobfile->llama-stack) (3.21.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.25.3 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from blobfile->llama-stack) (2.3.0)\n",
      "Requirement already satisfied: lxml>=4.9 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from blobfile->llama-stack) (5.3.1)\n",
      "Requirement already satisfied: filelock>=3.0 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from blobfile->llama-stack) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from huggingface-hub->llama-stack) (2025.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from huggingface-hub->llama-stack) (24.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from jsonschema->llama-stack) (25.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from jsonschema->llama-stack) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from jsonschema->llama-stack) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from jsonschema->llama-stack) (0.23.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from prompt-toolkit->llama-stack) (0.2.13)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from requests->llama-stack) (3.4.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from rich->llama-stack) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from rich->llama-stack) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->llama-stack) (0.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from tqdm->llama-stack-client>=0.1.5rc3->llama-stack) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from jinja2->llama-models>=0.1.5rc3->llama-stack) (3.0.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from pandas->llama-stack-client>=0.1.5rc3->llama-stack) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from pandas->llama-stack-client>=0.1.5rc3->llama-stack) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from pandas->llama-stack-client>=0.1.5rc3->llama-stack) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from pandas->llama-stack-client>=0.1.5rc3->llama-stack) (2025.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from tiktoken->llama-models>=0.1.5rc3->llama-stack) (2024.11.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama-stack-client>=0.1.5rc3->llama-stack) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install groq\n",
    "!pip install -U llama-stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'UV_SYSTEM_PYTHON' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!UV_SYSTEM_PYTHON=1 llama stack build --template together --image-type venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yt-dlp\n",
      "  Downloading yt_dlp-2025.2.19-py3-none-any.whl.metadata (171 kB)\n",
      "     ---------------------------------------- 0.0/171.9 kB ? eta -:--:--\n",
      "     -- ------------------------------------- 10.2/171.9 kB ? eta -:--:--\n",
      "     -------- ---------------------------- 41.0/171.9 kB 393.8 kB/s eta 0:00:01\n",
      "     ------------------- ----------------- 92.2/171.9 kB 751.6 kB/s eta 0:00:01\n",
      "     -------------------------------------- 171.9/171.9 kB 1.2 MB/s eta 0:00:00\n",
      "Collecting pytubefix\n",
      "  Downloading pytubefix-8.12.2-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting youtube-transcript-api\n",
      "  Downloading youtube_transcript_api-0.6.3-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting defusedxml<0.8.0,>=0.7.1 (from youtube-transcript-api)\n",
      "  Downloading defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from youtube-transcript-api) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from requests->youtube-transcript-api) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from requests->youtube-transcript-api) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from requests->youtube-transcript-api) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ranit\\personal_projects\\promptedu\\myenv\\lib\\site-packages (from requests->youtube-transcript-api) (2025.1.31)\n",
      "Downloading yt_dlp-2025.2.19-py3-none-any.whl (3.2 MB)\n",
      "   ---------------------------------------- 0.0/3.2 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.2/3.2 MB 7.6 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.5/3.2 MB 6.6 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.8/3.2 MB 6.6 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 1.1/3.2 MB 6.4 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.5/3.2 MB 6.8 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.7/3.2 MB 6.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 2.0/3.2 MB 6.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 2.4/3.2 MB 6.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.8/3.2 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.2/3.2 MB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.2/3.2 MB 6.8 MB/s eta 0:00:00\n",
      "Downloading pytubefix-8.12.2-py3-none-any.whl (730 kB)\n",
      "   ---------------------------------------- 0.0/730.7 kB ? eta -:--:--\n",
      "   ---------------------- ----------------- 409.6/730.7 kB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  716.8/730.7 kB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 730.7/730.7 kB 6.6 MB/s eta 0:00:00\n",
      "Downloading youtube_transcript_api-0.6.3-py3-none-any.whl (622 kB)\n",
      "   ---------------------------------------- 0.0/622.3 kB ? eta -:--:--\n",
      "   ------------------- -------------------- 307.2/622.3 kB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  614.4/622.3 kB 7.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 622.3/622.3 kB 7.8 MB/s eta 0:00:00\n",
      "Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Installing collected packages: yt-dlp, pytubefix, defusedxml, youtube-transcript-api\n",
      "Successfully installed defusedxml-0.7.1 pytubefix-8.12.2 youtube-transcript-api-0.6.3 yt-dlp-2025.2.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install yt-dlp pytubefix youtube-transcript-api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import pytubefix\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "@dataclass\n",
    "class VideoMetadata:\n",
    "    title : str\n",
    "    upload_data : str\n",
    "    duration_s : int\n",
    "    url : str\n",
    "\n",
    "def search_youtube(search_query, num_queries=2):\n",
    "    ydl_opts = {\n",
    "        \"default_search\": f\"ytsearch{num_queries}\",\n",
    "        \"quiet\": True,\n",
    "    }\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        search_results = ydl.extract_info(search_query, download=False)\n",
    "\n",
    "    return [VideoMetadata(x['title'], datetime.fromtimestamp(float(x['upload_date'])).strftime('%m/%d/%Y'), x['duration'], x['webpage_url']) for x in search_results['entries']]\n",
    "\n",
    "def get_transcript(url, fast=True):\n",
    "    if fast:\n",
    "        vid_id = pytubefix.YouTube(url).video_id\n",
    "    else:\n",
    "        with yt_dlp.YoutubeDL({'quiet':True}) as ydl:\n",
    "            vid_id = ydl.extract_info(url, download=False)['id']\n",
    "    return YouTubeTranscriptApi.get_transcript(vid_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from llama_stack_client.lib.agents.client_tool import client_tool\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@client_tool\n",
    "def get_video_transcript(video_url: str) -> str:\n",
    "    \"\"\"Simple Youtube Video Information tool that provides a timestamped video transcription.\n",
    "\n",
    "    :param video_url: The complete youtube video url to extract information from\n",
    "    :returns: String containing the video transcription in words and timestamps in bracketed numbers\n",
    "    \"\"\"\n",
    "    vid_id = pytubefix.YouTube(video_url).video_id\n",
    "    return YouTubeTranscriptApi.get_transcript(vid_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with open('api_key', 'r') as f:\n",
    "    os.environ['TOGETHER_API_KEY'] = f.readline()\n",
    "\n",
    "from llama_stack.distribution.library_client import LlamaStackAsLibraryClient\n",
    "\n",
    "client = LlamaStackAsLibraryClient(\"together\")\n",
    "client.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client.lib.agents.agent import Agent\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "from llama_stack_client.types.agent_create_params import AgentConfig\n",
    "from llama_stack_client.types import ToolDef, ToolInvocationResult\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "client_tools = [get_video_transcript]\n",
    "agent_config = AgentConfig(\n",
    "    model=model_id,\n",
    "    instructions=\"You are a helpful assistant. Please use the calculator tool to solve math problems\",\n",
    "    toolgroups=[],\n",
    "    client_tools=[\n",
    "            client_tool.get_tool_definition() for client_tool in client_tools\n",
    "        ],\n",
    "    tool_choice=\"auto\",\n",
    "    enable_session_persistence=True,\n",
    ")\n",
    "\n",
    "agent = Agent(client, agent_config, client_tools)\n",
    "\n",
    "user_prompts = [\n",
    "    \"Hello\",\n",
    "    \"Can you help me summarize this youtube video: https://www.youtube.com/watch?v=WsQQvHm4lSw?\",\n",
    "]\n",
    "\n",
    "session_id = agent.create_session(\"test-session\")\n",
    "for prompt in user_prompts:\n",
    "    response = agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=session_id,\n",
    "    )\n",
    "\n",
    "    for log in EventLogger().log(response):\n",
    "        log.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I'm trying to understand why low latency is important for large language models (LLMs). I remember reading that latency refers to the delay before a response is received, so low latency means faster responses. But why is that a big deal for LLMs?\n",
      "\n",
      "First, I think about where LLMs are used. They're in things like chatbots, virtual assistants, and maybe even in real-time applications. So if someone is using a chatbot and asks a question, they don't want to wait a long time for an answer. If the latency is high, the user experience would be slow and frustrating. That makes sense. People expect quick responses, especially when they're interacting in real-time, like in a conversation.\n",
      "\n",
      "Then there's real-time applications. I'm not entirely sure what qualifies as a real-time application, but maybe things like live translation or live subtitles. If you're translating speech in real-time, any delay could make the translation useless because the speaker has already moved on. So low latency is crucial there to keep up with the speaker's pace.\n",
      "\n",
      "Another point I remember is about user engagement. If an application is slow, people might get annoyed and stop using it. For example, if you're using a voice assistant to control smart home devices, a delay in response could be frustrating. You say \"turn on the lights,\" and if it takes a few seconds, it feels unresponsive. Low latency would make the interaction feel smoother and more natural.\n",
      "\n",
      "I also think about scalability. If an LLM is used in a system that handles many requests at once, like a customer service chatbot on a website, high latency could bottleneck the system. If each request takes longer, the system can't handle as many users at the same time. Low latency would allow the system to process more requests quickly, making it more scalable and efficient.\n",
      "\n",
      "Energy efficiency comes to mind too. If the model responds faster, maybe it doesn't use as much power. I'm not entirely sure how that works, but processing quicker could mean the hardware doesn't have to work as hard or for as long, which would save energy. That's probably important for data centers and for devices that run on batteries.\n",
      "\n",
      "Competitive advantage is another factor. If a company has an LLM with lower latency, their services might be more attractive. For example, in search engines, faster results can lead to better user satisfaction and more usage. So companies might invest in low latency to stay ahead of competitors.\n",
      "\n",
      "I'm trying to think of other areas. Maybe in healthcare, where quick responses can be critical. If a doctor is using an LLM to get information during an emergency, delays could be dangerous. So low latency is not just about convenience but could affect outcomes in sensitive fields.\n",
      "\n",
      "Also, in gaming or virtual reality, interactions need to be real-time. If an LLM is used to generate NPC dialogue or responses, high latency could break the immersion. Players expect immediate reactions, so low latency would make the experience better.\n",
      "\n",
      "I wonder about the technical side. How is low latency achieved? Maybe through optimizations in the model architecture, like pruning or quantization, or using faster hardware. But that's more about implementation. The importance is about the benefits it brings to users and applications.\n",
      "\n",
      "So, putting it all together, low latency in LLMs is important because it improves user experience, enables real-time applications, increases system efficiency, supports scalability, reduces energy consumption, provides a competitive edge, and is crucial for critical applications where delays can have significant consequences. Each of these points contributes to making LLMs more effective and widely applicable across different industries and use cases.\n",
      "</think>\n",
      "\n",
      "Low latency in Large Language Models (LLMs) is crucial for several reasons, each contributing to enhanced performance and user satisfaction across various applications:\n",
      "\n",
      "1. **User Experience**: Fast response times are essential for interactive applications like chatbots and virtual assistants. Low latency ensures quick answers, preventing frustration and drop-offs, thus improving engagement.\n",
      "\n",
      "2. **Real-Time Applications**: In applications such as live translation or subtitles, low latency is vital to maintain synchronicity with the speaker, ensuring the translation remains relevant and usable.\n",
      "\n",
      "3. **Scalability and Efficiency**: Systems handling multiple requests, like customer service chatbots, benefit from low latency as it allows processing more requests simultaneously, enhancing scalability and efficiency.\n",
      "\n",
      "4. **Energy Efficiency**: Faster processing can reduce power consumption, as hardware doesn't need to work as hard or for as long, benefiting data centers and battery-operated devices.\n",
      "\n",
      "5. **Competitive Advantage**: Companies with low-latency LLMs can offer superior services, attracting more users and staying ahead in competitive markets, such as search engines.\n",
      "\n",
      "6. **Critical Applications**: In fields like healthcare and emergency services, quick responses can be critical, potentially affecting outcomes and safety.\n",
      "\n",
      "7. **Gaming and Virtual Reality**: Low latency ensures real-time interactions, crucial for immersive experiences in gaming and virtual reality where immediate responses are expected.\n",
      "\n",
      "In summary, low latency enhances user experience, enables real-time functionalities, improves efficiency, supports scalability, reduces energy use, offers competitive advantages, and is crucial for critical applications, making LLMs more effective and versatile across industries.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "with open('api_key', 'r') as f:\n",
    "    api_key = f.readline()\n",
    "\n",
    "client = Groq(\n",
    "    api_key=api_key,  # This is the default and can be omitted\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of low latency LLMs\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"deepseek-r1-distill-llama-70b\",\n",
    ")\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
