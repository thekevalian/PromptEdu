{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Setup Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting groq\n",
      "  Downloading groq-0.18.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from groq) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from groq) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from groq) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /home/vkeval/anaconda3/lib/python3.12/site-packages (from groq) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from groq) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->groq) (3.7)\n",
      "Requirement already satisfied: certifi in /home/vkeval/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /home/vkeval/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->groq) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->groq) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->groq) (2.20.1)\n",
      "Downloading groq-0.18.0-py3-none-any.whl (121 kB)\n",
      "Installing collected packages: groq\n",
      "Successfully installed groq-0.18.0\n",
      "Collecting llama-stack\n",
      "  Downloading llama_stack-0.1.5.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting blobfile (from llama-stack)\n",
      "  Downloading blobfile-3.0.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting fire (from llama-stack)\n",
      "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: httpx in /home/vkeval/anaconda3/lib/python3.12/site-packages (from llama-stack) (0.27.0)\n",
      "Collecting huggingface-hub (from llama-stack)\n",
      "  Downloading huggingface_hub-0.29.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: jsonschema in /home/vkeval/anaconda3/lib/python3.12/site-packages (from llama-stack) (4.23.0)\n",
      "Collecting llama-models>=0.1.5rc3 (from llama-stack)\n",
      "  Downloading llama_models-0.1.5-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting llama-stack-client>=0.1.5rc3 (from llama-stack)\n",
      "  Downloading llama_stack_client-0.1.5-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: prompt-toolkit in /home/vkeval/anaconda3/lib/python3.12/site-packages (from llama-stack) (3.0.43)\n",
      "Requirement already satisfied: python-dotenv in /home/vkeval/anaconda3/lib/python3.12/site-packages (from llama-stack) (0.21.0)\n",
      "Requirement already satisfied: pydantic>=2 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from llama-stack) (2.8.2)\n",
      "Requirement already satisfied: requests in /home/vkeval/anaconda3/lib/python3.12/site-packages (from llama-stack) (2.32.3)\n",
      "Requirement already satisfied: rich in /home/vkeval/anaconda3/lib/python3.12/site-packages (from llama-stack) (13.7.1)\n",
      "Requirement already satisfied: setuptools in /home/vkeval/anaconda3/lib/python3.12/site-packages (from llama-stack) (75.1.0)\n",
      "Collecting termcolor (from llama-stack)\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: PyYAML in /home/vkeval/anaconda3/lib/python3.12/site-packages (from llama-models>=0.1.5rc3->llama-stack) (6.0.1)\n",
      "Requirement already satisfied: jinja2 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from llama-models>=0.1.5rc3->llama-stack) (3.1.4)\n",
      "Collecting tiktoken (from llama-models>=0.1.5rc3->llama-stack)\n",
      "  Downloading tiktoken-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: Pillow in /home/vkeval/anaconda3/lib/python3.12/site-packages (from llama-models>=0.1.5rc3->llama-stack) (10.4.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from llama-stack-client>=0.1.5rc3->llama-stack) (4.2.0)\n",
      "Requirement already satisfied: click in /home/vkeval/anaconda3/lib/python3.12/site-packages (from llama-stack-client>=0.1.5rc3->llama-stack) (8.1.7)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from llama-stack-client>=0.1.5rc3->llama-stack) (1.9.0)\n",
      "Requirement already satisfied: pandas in /home/vkeval/anaconda3/lib/python3.12/site-packages (from llama-stack-client>=0.1.5rc3->llama-stack) (2.2.2)\n",
      "Collecting pyaml (from llama-stack-client>=0.1.5rc3->llama-stack)\n",
      "  Downloading pyaml-25.1.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: sniffio in /home/vkeval/anaconda3/lib/python3.12/site-packages (from llama-stack-client>=0.1.5rc3->llama-stack) (1.3.0)\n",
      "Requirement already satisfied: tqdm in /home/vkeval/anaconda3/lib/python3.12/site-packages (from llama-stack-client>=0.1.5rc3->llama-stack) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from llama-stack-client>=0.1.5rc3->llama-stack) (4.11.0)\n",
      "Requirement already satisfied: certifi in /home/vkeval/anaconda3/lib/python3.12/site-packages (from httpx->llama-stack) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /home/vkeval/anaconda3/lib/python3.12/site-packages (from httpx->llama-stack) (1.0.2)\n",
      "Requirement already satisfied: idna in /home/vkeval/anaconda3/lib/python3.12/site-packages (from httpx->llama-stack) (3.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-stack) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from pydantic>=2->llama-stack) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from pydantic>=2->llama-stack) (2.20.1)\n",
      "Collecting pycryptodomex>=3.8 (from blobfile->llama-stack)\n",
      "  Downloading pycryptodomex-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.25.3 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from blobfile->llama-stack) (2.2.3)\n",
      "Requirement already satisfied: lxml>=4.9 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from blobfile->llama-stack) (5.2.1)\n",
      "Requirement already satisfied: filelock>=3.0 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from blobfile->llama-stack) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from huggingface-hub->llama-stack) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from huggingface-hub->llama-stack) (24.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from jsonschema->llama-stack) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from jsonschema->llama-stack) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from jsonschema->llama-stack) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from jsonschema->llama-stack) (0.10.6)\n",
      "Requirement already satisfied: wcwidth in /home/vkeval/anaconda3/lib/python3.12/site-packages (from prompt-toolkit->llama-stack) (0.2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from requests->llama-stack) (3.3.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from rich->llama-stack) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from rich->llama-stack) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->llama-stack) (0.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from jinja2->llama-models>=0.1.5rc3->llama-stack) (2.1.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from pandas->llama-stack-client>=0.1.5rc3->llama-stack) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from pandas->llama-stack-client>=0.1.5rc3->llama-stack) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from pandas->llama-stack-client>=0.1.5rc3->llama-stack) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from pandas->llama-stack-client>=0.1.5rc3->llama-stack) (2023.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from tiktoken->llama-models>=0.1.5rc3->llama-stack) (2024.9.11)\n",
      "Requirement already satisfied: six>=1.5 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->llama-stack-client>=0.1.5rc3->llama-stack) (1.16.0)\n",
      "Downloading llama_stack-0.1.5.1-py3-none-any.whl (736 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m736.5/736.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llama_models-0.1.5-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading llama_stack_client-0.1.5-py3-none-any.whl (369 kB)\n",
      "Downloading blobfile-3.0.0-py3-none-any.whl (75 kB)\n",
      "Downloading huggingface_hub-0.29.1-py3-none-any.whl (468 kB)\n",
      "Downloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading pycryptodomex-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyaml-25.1.0-py3-none-any.whl (26 kB)\n",
      "Downloading tiktoken-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: fire\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=1df4cae3f57201a32b840fb801f31a009d1a711503748cc6ad81f6ca8164810f\n",
      "  Stored in directory: /home/vkeval/.cache/pip/wheels/9e/5b/45/29f72e55d87a29426b04b3cfdf20325c079eb97ab74f59017d\n",
      "Successfully built fire\n",
      "Installing collected packages: termcolor, pycryptodomex, pyaml, tiktoken, huggingface-hub, fire, blobfile, llama-stack-client, llama-models, llama-stack\n",
      "Successfully installed blobfile-3.0.0 fire-0.7.0 huggingface-hub-0.29.1 llama-models-0.1.5 llama-stack-0.1.5.1 llama-stack-client-0.1.5 pyaml-25.1.0 pycryptodomex-3.21.0 termcolor-2.5.0 tiktoken-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install groq\n",
    "!pip install -U llama-stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uv is not installed, trying to install it.\n",
      "Collecting uv\n",
      "  Downloading uv-0.6.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Downloading uv-0.6.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.2/16.2 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: uv\n",
      "Successfully installed uv-0.6.3\n",
      "Installing dependencies in system Python environment\n",
      "\u001b[2mUsing Python 3.12.7 environment at: /home/vkeval/anaconda3\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 1.07s\u001b[0m\u001b[0m\n",
      "Installing pip dependencies\n",
      "\u001b[2mUsing Python 3.12.7 environment at: /home/vkeval/anaconda3\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m119 packages\u001b[0m \u001b[2min 1.58s\u001b[0m\u001b[0m                                       \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m50 packages\u001b[0m \u001b[2min 9.35s\u001b[0m\u001b[0m                                            \n",
      "\u001b[2mUninstalled \u001b[1m3 packages\u001b[0m \u001b[2min 99ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m50 packages\u001b[0m \u001b[2min 1.71s\u001b[0m\u001b[0m                              \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiosqlite\u001b[0m\u001b[2m==0.21.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1manyio\u001b[0m\u001b[2m==4.2.0 (from file:///croot/anyio_1706220167567/work)\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1manyio\u001b[0m\u001b[2m==4.8.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mautoevals\u001b[0m\u001b[2m==0.0.121\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mbackoff\u001b[0m\u001b[2m==2.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mbraintrust-core\u001b[0m\u001b[2m==0.0.58\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mchevron\u001b[0m\u001b[2m==0.14.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mchromadb-client\u001b[0m\u001b[2m==0.6.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdatasets\u001b[0m\u001b[2m==3.3.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdeprecated\u001b[0m\u001b[2m==1.2.18\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdnspython\u001b[0m\u001b[2m==2.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1meval-type-backport\u001b[0m\u001b[2m==0.2.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfaiss-cpu\u001b[0m\u001b[2m==1.10.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfastapi\u001b[0m\u001b[2m==0.115.10\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgoogleapis-common-protos\u001b[0m\u001b[2m==1.68.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgrpcio\u001b[0m\u001b[2m==1.70.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpx-sse\u001b[0m\u001b[2m==0.4.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjiter\u001b[0m\u001b[2m==0.8.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlevenshtein\u001b[0m\u001b[2m==0.26.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmcp\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmonotonic\u001b[0m\u001b[2m==1.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmultiprocess\u001b[0m\u001b[2m==0.70.16\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopenai\u001b[0m\u001b[2m==1.65.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-api\u001b[0m\u001b[2m==1.30.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-common\u001b[0m\u001b[2m==1.30.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-grpc\u001b[0m\u001b[2m==1.30.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-http\u001b[0m\u001b[2m==1.30.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-proto\u001b[0m\u001b[2m==1.30.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-sdk\u001b[0m\u001b[2m==1.30.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-semantic-conventions\u001b[0m\u001b[2m==0.51b0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1morjson\u001b[0m\u001b[2m==3.10.15\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mposthog\u001b[0m\u001b[2m==3.18.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==4.25.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==5.29.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpsycopg2-binary\u001b[0m\u001b[2m==2.9.10\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-settings\u001b[0m\u001b[2m==2.8.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpymongo\u001b[0m\u001b[2m==4.11.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpypdf\u001b[0m\u001b[2m==5.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrapidfuzz\u001b[0m\u001b[2m==3.12.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mredis\u001b[0m\u001b[2m==5.2.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mrich\u001b[0m\u001b[2m==13.7.1 (from file:///croot/rich_1720637495510/work)\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrich\u001b[0m\u001b[2m==13.9.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msafetensors\u001b[0m\u001b[2m==0.5.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msentencepiece\u001b[0m\u001b[2m==0.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mshellingham\u001b[0m\u001b[2m==1.5.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msse-starlette\u001b[0m\u001b[2m==2.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mstarlette\u001b[0m\u001b[2m==0.46.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtogether\u001b[0m\u001b[2m==1.3.14\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.21.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.49.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyper\u001b[0m\u001b[2m==0.15.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1muvicorn\u001b[0m\u001b[2m==0.34.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mxxhash\u001b[0m\u001b[2m==3.5.0\u001b[0m\n",
      "sentence-transformers --no-deps\n",
      "\u001b[2mUsing Python 3.12.7 environment at: /home/vkeval/anaconda3\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 119ms\u001b[0m\u001b[0m                                          \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 290ms\u001b[0m\u001b[0m                                              \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 101ms\u001b[0m\u001b[0m==3.4.1                         \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msentence-transformers\u001b[0m\u001b[2m==3.4.1\u001b[0m\n",
      "torch torchvision --index-url https://download.pytorch.org/whl/cpu\n",
      "\u001b[2mUsing Python 3.12.7 environment at: /home/vkeval/anaconda3\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m13 packages\u001b[0m \u001b[2min 1.14s\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m3 packages\u001b[0m \u001b[2min 27.73s\u001b[0m\u001b[0m                                            \n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 673ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m3 packages\u001b[0m \u001b[2min 11.60s\u001b[0m\u001b[0m                              \u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.13.2 (from file:///croot/sympy_1724938189289/work)\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.13.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.6.0+cpu\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.21.0+cpu\u001b[0m\n",
      "\u001b[32mBuild Successful!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!UV_SYSTEM_PYTHON=1 llama stack build --template together --image-type venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yt-dlp\n",
      "  Downloading yt_dlp-2025.2.19-py3-none-any.whl.metadata (171 kB)\n",
      "Collecting pytubefix\n",
      "  Downloading pytubefix-8.12.2-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting youtube-transcript-api\n",
      "  Downloading youtube_transcript_api-0.6.3-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from youtube-transcript-api) (0.7.1)\n",
      "Requirement already satisfied: requests in /home/vkeval/anaconda3/lib/python3.12/site-packages (from youtube-transcript-api) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from requests->youtube-transcript-api) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from requests->youtube-transcript-api) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from requests->youtube-transcript-api) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vkeval/anaconda3/lib/python3.12/site-packages (from requests->youtube-transcript-api) (2024.8.30)\n",
      "Downloading yt_dlp-2025.2.19-py3-none-any.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytubefix-8.12.2-py3-none-any.whl (730 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m730.7/730.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading youtube_transcript_api-0.6.3-py3-none-any.whl (622 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.3/622.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: yt-dlp, pytubefix, youtube-transcript-api\n",
      "Successfully installed pytubefix-8.12.2 youtube-transcript-api-0.6.3 yt-dlp-2025.2.19\n"
     ]
    }
   ],
   "source": [
    "!pip install yt-dlp pytubefix youtube-transcript-api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "@dataclass\n",
    "class VideoMetadata:\n",
    "    title : str\n",
    "    upload_data : str\n",
    "    duration_s : int\n",
    "    url : str\n",
    "\n",
    "def search_youtube(search_query, num_queries=2):\n",
    "    ydl_opts = {\n",
    "        \"default_search\": f\"ytsearch{num_queries}\",\n",
    "        \"quiet\": True,\n",
    "    }\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        search_results = ydl.extract_info(search_query, download=False)\n",
    "\n",
    "    return [VideoMetadata(x['title'], datetime.fromtimestamp(float(x['upload_date'])).strftime('%m/%d/%Y'), x['duration'], x['webpage_url']) for x in search_results['entries']]\n",
    "\n",
    "def get_transcript(url, fast=True):\n",
    "    if fast:\n",
    "        vid_id = pytubefix.YouTube(url).video_id\n",
    "    else:\n",
    "        with yt_dlp.YoutubeDL({'quiet':True}) as ydl:\n",
    "            vid_id = ydl.extract_info(url, download=False)['id']\n",
    "    return YouTubeTranscriptApi.get_transcript(vid_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I'm trying to understand why low latency is important for large language models (LLMs). I remember reading that latency refers to the delay before a response is received, so low latency means faster responses. But why is that a big deal for LLMs?\n",
      "\n",
      "First, I think about where LLMs are used. They're in things like chatbots, virtual assistants, and maybe even in real-time applications. So if someone is using a chatbot and asks a question, they don't want to wait a long time for an answer. If the latency is high, the user experience would be slow and frustrating. That makes sense. People expect quick responses, especially when they're interacting in real-time, like in a conversation.\n",
      "\n",
      "Then there's real-time applications. I'm not entirely sure what qualifies as a real-time application, but maybe things like live translation or live subtitles. If you're translating speech in real-time, any delay could make the translation useless because the speaker has already moved on. So low latency is crucial there to keep up with the speaker's pace.\n",
      "\n",
      "Another point I remember is about user engagement. If an application is slow, people might get annoyed and stop using it. For example, if you're using a voice assistant to control smart home devices, a delay in response could be frustrating. You say \"turn on the lights,\" and if it takes a few seconds, it feels unresponsive. Low latency would make the interaction feel smoother and more natural.\n",
      "\n",
      "I also think about scalability. If an LLM is used in a system that handles many requests at once, like a customer service chatbot on a website, high latency could bottleneck the system. If each request takes longer, the system can't handle as many users at the same time. Low latency would allow the system to process more requests quickly, making it more scalable and efficient.\n",
      "\n",
      "Energy efficiency comes to mind too. If the model responds faster, maybe it doesn't use as much power. I'm not entirely sure how that works, but processing quicker could mean the hardware doesn't have to work as hard or for as long, which would save energy. That's probably important for data centers and for devices that run on batteries.\n",
      "\n",
      "Competitive advantage is another factor. If a company has an LLM with lower latency, their services might be more attractive. For example, in search engines, faster results can lead to better user satisfaction and more usage. So companies might invest in low latency to stay ahead of competitors.\n",
      "\n",
      "I'm trying to think of other areas. Maybe in healthcare, where quick responses can be critical. If a doctor is using an LLM to get information during an emergency, delays could be dangerous. So low latency is not just about convenience but could affect outcomes in sensitive fields.\n",
      "\n",
      "Also, in gaming or virtual reality, interactions need to be real-time. If an LLM is used to generate NPC dialogue or responses, high latency could break the immersion. Players expect immediate reactions, so low latency would make the experience better.\n",
      "\n",
      "I wonder about the technical side. How is low latency achieved? Maybe through optimizations in the model architecture, like pruning or quantization, or using faster hardware. But that's more about implementation. The importance is about the benefits it brings to users and applications.\n",
      "\n",
      "So, putting it all together, low latency in LLMs is important because it improves user experience, enables real-time applications, increases system efficiency, supports scalability, reduces energy consumption, provides a competitive edge, and is crucial for critical applications where delays can have significant consequences. Each of these points contributes to making LLMs more effective and widely applicable across different industries and use cases.\n",
      "</think>\n",
      "\n",
      "Low latency in Large Language Models (LLMs) is crucial for several reasons, each contributing to enhanced performance and user satisfaction across various applications:\n",
      "\n",
      "1. **User Experience**: Fast response times are essential for interactive applications like chatbots and virtual assistants. Low latency ensures quick answers, preventing frustration and drop-offs, thus improving engagement.\n",
      "\n",
      "2. **Real-Time Applications**: In applications such as live translation or subtitles, low latency is vital to maintain synchronicity with the speaker, ensuring the translation remains relevant and usable.\n",
      "\n",
      "3. **Scalability and Efficiency**: Systems handling multiple requests, like customer service chatbots, benefit from low latency as it allows processing more requests simultaneously, enhancing scalability and efficiency.\n",
      "\n",
      "4. **Energy Efficiency**: Faster processing can reduce power consumption, as hardware doesn't need to work as hard or for as long, benefiting data centers and battery-operated devices.\n",
      "\n",
      "5. **Competitive Advantage**: Companies with low-latency LLMs can offer superior services, attracting more users and staying ahead in competitive markets, such as search engines.\n",
      "\n",
      "6. **Critical Applications**: In fields like healthcare and emergency services, quick responses can be critical, potentially affecting outcomes and safety.\n",
      "\n",
      "7. **Gaming and Virtual Reality**: Low latency ensures real-time interactions, crucial for immersive experiences in gaming and virtual reality where immediate responses are expected.\n",
      "\n",
      "In summary, low latency enhances user experience, enables real-time functionalities, improves efficiency, supports scalability, reduces energy use, offers competitive advantages, and is crucial for critical applications, making LLMs more effective and versatile across industries.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "with open('api_key', 'r') as f:\n",
    "    api_key = f.readline()\n",
    "\n",
    "client = Groq(\n",
    "    api_key=api_key,  # This is the default and can be omitted\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of low latency LLMs\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"deepseek-r1-distill-llama-70b\",\n",
    ")\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
